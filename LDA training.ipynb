{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "%matplotlib inline  \n",
    "import tweepy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('tweets_final3.csv', names = [\"username\", \"tweets_raw\", \"politics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Various methods for cleaning tweet strings to make them better suited for sentiment analysis\n",
    "\n",
    "def remove_emojis(string):\n",
    "    \n",
    "    corrected = re.sub(r'\\\\x[\\w+]{2}', '', string)\n",
    "    \n",
    "    return corrected\n",
    "\n",
    "def correct_apostrophes(string):\n",
    "    \n",
    "    corrected = re.sub(r'\\\\xe2\\\\x80\\\\x99', '\\'', string)\n",
    "    \n",
    "    corrected = re.sub(r'\\&amp;', 'and', string)\n",
    "    \n",
    "    return corrected\n",
    "\n",
    "def remove_byte_encoding(string):\n",
    "    return string[2:]\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \n",
    "            \n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        return re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet)\n",
    "    \n",
    "def clean_tweet(tweet):\n",
    "    \n",
    "        parts = tweet.split('.')\n",
    "        \n",
    "        for i in range(len(parts)):\n",
    "            parts[i] = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", parts[i])\n",
    "            \n",
    "        cleaned_tweet = \" \".join(parts)\n",
    "        \n",
    "        return cleaned_tweet\n",
    "    \n",
    "def clean_string(string):\n",
    "    \n",
    "    temp = remove_byte_encoding(string)\n",
    "    temp = correct_apostrophes(temp)\n",
    "    temp = remove_emojis(temp)\n",
    "    final = clean_tweet(temp)\n",
    "    \n",
    "    return final\n",
    "\n",
    "def clean_dataframe_tweets(dataframe):\n",
    "    \n",
    "    new_strings = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        new_strings.append(clean_string(row['tweets_raw']))\n",
    "    \n",
    "    return new_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned = clean_dataframe_tweets(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df['tweets_clean'] = cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import string\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    \n",
    "    doc_clean = [clean(doc).split() for doc in corpus]  \n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    \n",
    "    return doc_term_matrix, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_LDA_model(corpus, num_topics=15):\n",
    "    \n",
    "    new_corpus, dictionary = preprocess_corpus(corpus)\n",
    "    \n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(new_corpus, num_topics=num_topics, id2word = dictionary, passes=2)\n",
    "    \n",
    "    return ldamodel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = generate_LDA_model(tweets_df['tweets_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_topic_distribution(string, ldamodel):\n",
    "    \n",
    "    tokens = tokenizer.tokenize(string)\n",
    "    \n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    texts = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    bow = ldamodel.id2word.doc2bow(texts)\n",
    "    \n",
    "    return ldamodel.get_document_topics(bow)\n",
    "    \n",
    "\n",
    "def tweet_to_tdistrib(tweet, ldamodel, second=False):\n",
    "    t_distrib = get_topic_distribution(tweet, ldamodel)\n",
    "    \n",
    "    if second:\n",
    "        return pd.Series(dist_to_row(t_distrib), index = ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19'])\n",
    "    \n",
    "    return pd.Series(dist_to_row(t_distrib))\n",
    "\n",
    "def dist_to_row(dist, num_topics=15):\n",
    "    \n",
    "    row = []\n",
    "    \n",
    "    for i in range(num_topics):\n",
    "        row.append(0)\n",
    "    \n",
    "    for topic in dist:\n",
    "        row[topic[0]] = topic[1]\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_to_row(tweet, lda1):\n",
    "    \n",
    "    new_row = tweet_to_tdistrib(tweet, lda1)\n",
    "\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topic_features(dataframe, ldamodel):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        tweet = row['tweets_clean']\n",
    "        \n",
    "        new_row = tweet_to_row(tweet, ldamodel)\n",
    "        \n",
    "        df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics = get_topic_features(tweets_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.14609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152166</td>\n",
       "      <td>0.197141</td>\n",
       "      <td>0.215345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.328630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349944</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.301475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063312</td>\n",
       "      <td>0.109490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078796</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290406</td>\n",
       "      <td>0.642928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.197156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068353</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.062705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.186857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2        3         4         5         6   \\\n",
       "0  0.000000  0.087050  0.000000  0.14609  0.000000  0.047136  0.000000   \n",
       "1  0.000000  0.073384  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.349944  0.00000  0.301475  0.000000  0.063312   \n",
       "3  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "4  0.197156  0.000000  0.068353  0.00000  0.062705  0.000000  0.000000   \n",
       "\n",
       "         7    8         9         10        11        12        13        14  \n",
       "0  0.576480  0.0  0.055894  0.000000  0.061262  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.0  0.152166  0.197141  0.215345  0.000000  0.000000  0.328630  \n",
       "2  0.109490  0.0  0.000000  0.000000  0.068411  0.000000  0.078796  0.000000  \n",
       "3  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.290406  0.642928  \n",
       "4  0.447892  0.0  0.000000  0.000000  0.000000  0.186857  0.000000  0.000000  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment(string):\n",
    "    text = TextBlob(string)\n",
    "    \n",
    "    return text.sentiment\n",
    "\n",
    "def get_dataframe_sentiments(dataframe):\n",
    "    \n",
    "    sentiments = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        sentiments.append(get_sentiment(row['tweets_clean']))\n",
    "        \n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiments = get_dataframe_sentiments(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66572, 4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'Enjoyed assembling snack packages with colleagues for our deployed service members! Small token of our great appreciation for your service &amp; commitmet to our nation \\\\xf0\\\\x9f\\\\x87\\\\xba\\\\xf0\\\\x9f\\\\x87\\\\xb8 @the_USO #BeTheForce https://t.co/06zJWntkIv'\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['tweets_raw'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.031*\"co\" + 0.010*\"infrastructure\" + 0.008*\"bush\" + 0.008*\"de\" + 0.007*\"la\" + 0.007*\"development\" + 0.006*\"new\" + 0.006*\"en\" + 0.006*\"first\" + 0.006*\"community\"'),\n",
       " (1,\n",
       "  '0.048*\"co\" + 0.024*\"thank\" + 0.020*\"woman\" + 0.018*\"service\" + 0.015*\"honor\" + 0.014*\"day\" + 0.014*\"today\" + 0.013*\"work\" + 0.013*\"year\" + 0.011*\"life\"'),\n",
       " (2,\n",
       "  '0.046*\"co\" + 0.024*\"school\" + 0.023*\"student\" + 0.021*\"high\" + 0.017*\"texas\" + 0.017*\"congressional\" + 0.011*\"year\" + 0.010*\"capitol\" + 0.009*\"art\" + 0.008*\"irs\"'),\n",
       " (3,\n",
       "  '0.044*\"taxreform\" + 0.023*\"co\" + 0.022*\"h\" + 0.020*\"r\" + 0.014*\"act\" + 0.013*\"vote\" + 0.013*\"bill\" + 0.012*\"passed\" + 0.011*\"house\" + 0.007*\"floor\"'),\n",
       " (4,\n",
       "  '0.029*\"co\" + 0.020*\"2\" + 0.018*\"year\" + 0.018*\"american\" + 0.017*\"tax\" + 0.017*\"1\" + 0.013*\"taxcutsandjobsact\" + 0.012*\"4\" + 0.011*\"million\" + 0.010*\"000\"'),\n",
       " (5,\n",
       "  '0.034*\"co\" + 0.011*\"opioid\" + 0.009*\"crisis\" + 0.009*\"people\" + 0.009*\"make\" + 0.008*\"american\" + 0.008*\"school\" + 0.008*\"country\" + 0.008*\"work\" + 0.008*\"need\"'),\n",
       " (6,\n",
       "  '0.070*\"tax\" + 0.056*\"n\" + 0.055*\"co\" + 0.027*\"reform\" + 0.019*\"cut\" + 0.016*\"job\" + 0.014*\"american\" + 0.013*\"family\" + 0.009*\"benefit\" + 0.008*\"pro\"'),\n",
       " (7,\n",
       "  '0.031*\"co\" + 0.013*\"family\" + 0.013*\"today\" + 0.013*\"forward\" + 0.012*\"officer\" + 0.012*\"law\" + 0.012*\"prayer\" + 0.011*\"friend\" + 0.010*\"life\" + 0.010*\"enforcement\"'),\n",
       " (8,\n",
       "  '0.050*\"co\" + 0.020*\"news\" + 0.020*\"north\" + 0.016*\"new\" + 0.015*\"great\" + 0.012*\"seeing\" + 0.009*\"congressman\" + 0.009*\"good\" + 0.008*\"korea\" + 0.008*\"ohio\"'),\n",
       " (9,\n",
       "  '0.041*\"co\" + 0.038*\"business\" + 0.029*\"job\" + 0.020*\"small\" + 0.019*\"economy\" + 0.013*\"millennials\" + 0.012*\"growth\" + 0.011*\"gopfuture\" + 0.011*\"betterway\" + 0.010*\"new\"'),\n",
       " (10,\n",
       "  '0.028*\"co\" + 0.022*\"chairman\" + 0.016*\"president\" + 0.013*\"trump\" + 0.012*\"state\" + 0.010*\"u\" + 0.009*\"subcommittee\" + 0.009*\"american\" + 0.009*\"israel\" + 0.009*\"security\"'),\n",
       " (11,\n",
       "  '0.081*\"co\" + 0.030*\"great\" + 0.020*\"today\" + 0.015*\"thanks\" + 0.014*\"meeting\" + 0.013*\"happy\" + 0.011*\"discus\" + 0.011*\"morning\" + 0.010*\"day\" + 0.009*\"office\"'),\n",
       " (12,\n",
       "  '0.042*\"co\" + 0.024*\"house\" + 0.016*\"bill\" + 0.014*\"act\" + 0.011*\"today\" + 0.010*\"bipartisan\" + 0.010*\"legislation\" + 0.009*\"congress\" + 0.007*\"support\" + 0.007*\"committee\"'),\n",
       " (13,\n",
       "  '0.030*\"co\" + 0.019*\"bill\" + 0.016*\"program\" + 0.016*\"health\" + 0.013*\"help\" + 0.013*\"care\" + 0.012*\"veteran\" + 0.010*\"need\" + 0.009*\"community\" + 0.009*\"funding\"'),\n",
       " (14,\n",
       "  '0.122*\"co\" + 0.031*\"live\" + 0.030*\"hearing\" + 0.030*\"watch\" + 0.021*\"statement\" + 0.021*\"gt\" + 0.019*\"read\" + 0.019*\"tune\" + 0.017*\"full\" + 0.013*\"code\"')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Touching and productive conversation w  Florida student survivors of gun violence  Were listening  And agree we need ACTION to stop this epidemic  I stand with our young leaders in support of sensible gun laws   n NeverAgain   co gCiOtA6qeF '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['tweets_clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = {\n",
    "           'Democrat': 0,\n",
    "           'Republican': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df_topics1\n",
    "y = tweets_df['politics'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5296070419996395"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a logistic regression model, and fit with X and y\n",
    "model = LogisticRegression()\n",
    "model.fit(X,y)\n",
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_sentiment(dataframe, sentiment):\n",
    "    df = dataframe.mul(sentiment, axis=0)\n",
    "    return df\n",
    "\n",
    "def get_sentiments(dataframe):\n",
    "    \n",
    "    s = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        s.append(row['sentiment'][0])\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = get_sentiments(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_topics1 = apply_sentiment(df_topics, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
