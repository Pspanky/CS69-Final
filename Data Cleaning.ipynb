{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Authentication and setup for Tweepy\n",
    "\n",
    "consumer_key = 'aQzHCKeyTaBKwqJaaiwda4bkO'\n",
    "consumer_secret = 'JLRfDVG7k8GbbkkOZofL6Al69P2Ov1xhWRBAjFNFR4FwSKoXDj'\n",
    "access_token = '3125452793-u04kuuehiQIPh94QUelRV6RREMfOxoDBq7oznT7'\n",
    "access_token_secret = 'xcejx5JhpsvRvsWylOjXbeKyUce8AjxgsmFz54v5qdvrh'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "handles = pd.read_csv('TwitterHandles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function which, given a set of tweets from Tweepy API, will write them to a csv file\n",
    "\n",
    "def write_tweets_to_csv(tweets, csvWriter, political_affiliation):\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        csvWriter.writerow([tweet.user.screen_name, tweet.full_text.encode('utf-8'), political_affiliation])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for retrieving latest tweets of a given username\n",
    "\n",
    "def get_user_tweets(username, tweet_count = 200): \n",
    "    \n",
    "    tweets = api.user_timeline(screen_name = username, count = tweet_count, \n",
    "                          include_rts = False, tweet_mode='extended')\n",
    "    return tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main function for recording tweets from an account\n",
    "\n",
    "def record_tweets(username, csvWriter, politic):\n",
    "    \n",
    "    tweets = get_user_tweets(username)\n",
    "    \n",
    "    write_tweets_to_csv(tweets, csvWriter, politic)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a dataframe of politician usernames will c\n",
    "\n",
    "def get_tweets_from_dataframe(dataframe, csvWriter):\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        record_tweets(row['TwitterHandle'], csvWriter, row['Party'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a csv file\n",
    "csvFile = open('tweets_final3.csv', 'a')\n",
    "\n",
    "#Use csv Writer\n",
    "csvWriter = csv.writer(csvFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_tweets_from_dataframe(handles, csvWriter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the csv into a dataframe\n",
    "\n",
    "tweets_df1 = pd.read_csv('tweets_final3.csv', names = [\"username\", \"tweets\", \"politics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Various methods for cleaning tweet strings to make them better suited for sentiment analysis\n",
    "\n",
    "def remove_emojis(string):\n",
    "    \n",
    "    corrected = re.sub(r'\\\\x[\\w+]{2}', '', string)\n",
    "    \n",
    "    return corrected\n",
    "\n",
    "def correct_apostrophes(string):\n",
    "    \n",
    "    corrected = re.sub(r'\\\\xe2\\\\x80\\\\x99', '\\'', string)\n",
    "    \n",
    "    return corrected\n",
    "\n",
    "def remove_byte_encoding(string):\n",
    "    return string[2:]\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        return re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_string(string):\n",
    "    \n",
    "    #temp = remove_byte_encoding(string)\n",
    "    temp = correct_apostrophes(string)\n",
    "    temp = remove_emojis(temp)\n",
    "    final = clean_tweet(temp)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_dataframe_tweets(dataframe):\n",
    "    \n",
    "    new_strings = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        new_strings.append(clean_string(row['tweets_raw']))\n",
    "    \n",
    "    return new_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned = clean_dataframe_tweets(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df['tweets_clean'] = cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiments, hashtags, and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracts all hashtags from a string. Useful for getting hashtags from tweets\n",
    "\n",
    "def extract_hashtags(string):\n",
    "    hashtags = re.findall(r\"#(\\w+)\", string)\n",
    "    \n",
    "    return hashtags\n",
    "\n",
    "\n",
    "# Iterates over a dataframe and extracts all hashtags\n",
    "\n",
    "def find_hashtags(dataframe):\n",
    "    \n",
    "    h_list = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        hashtags = extract_hashtags(row['tweets_raw'])\n",
    "        \n",
    "        h_list.append(hashtags)\n",
    "    \n",
    "    return h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracts all hashtags from a string. Useful for getting hashtags from tweets\n",
    "\n",
    "def extract_mentions(string):\n",
    "    hashtags = re.findall(r\"@(\\w+)\", string)\n",
    "    \n",
    "    return hashtags\n",
    "\n",
    "\n",
    "# Iterates over a dataframe and extracts all hashtags\n",
    "\n",
    "def find_mentions(dataframe):\n",
    "    \n",
    "    h_list = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        hashtags = extract_mentions(row['tweets_raw'])\n",
    "        \n",
    "        h_list.append(hashtags)\n",
    "    \n",
    "    return h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract hashtags and add them as a separate column\n",
    "\n",
    "hashtags = find_hashtags(tweets_df)\n",
    "\n",
    "tweets_df['hashtags'] = hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract mentions and add them as a separate column\n",
    "\n",
    "mentions = find_mentions(tweets_df)\n",
    "\n",
    "tweets_df['mentions'] = mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sentiment(string):\n",
    "    text = TextBlob(string)\n",
    "    \n",
    "    return text.sentiment\n",
    "\n",
    "def get_dataframe_sentiments(dataframe):\n",
    "    \n",
    "    sentiments = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        sentiments.append(get_sentiment(row['tweets_clean']))\n",
    "        \n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiments = get_dataframe_sentiments(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df_d = tweets_df.head(n=33099)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df_r = tweets_df.iloc[33099:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct corpus from the tweets, grouped by politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = tweets_df['tweets_clean']\n",
    "s.str.cat(sep='. ')\n",
    "\n",
    "corpus = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = tweets_df_d['tweets_clean']\n",
    "s.str.cat(sep='. ')\n",
    "\n",
    "corpus_d = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = tweets_df_r['tweets_clean']\n",
    "\n",
    "corpus_r = s.str.cat(sep='. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in tweets_df_r['tweets_clean']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the object for LDA model\n",
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Running and Trainign LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word = dictionary, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.020*\"bill\" + 0.020*\"act\" + 0.015*\"house\" + 0.012*\"h\" + 0.011*\"today\"'), (1, '0.017*\"thank\" + 0.016*\"law\" + 0.012*\"officer\" + 0.010*\"enforcement\" + 0.010*\"community\"'), (2, '0.021*\"great\" + 0.011*\"thanks\" + 0.010*\"today\" + 0.010*\"forward\" + 0.008*\"meeting\"'), (3, '0.011*\"government\" + 0.010*\"health\" + 0.010*\"national\" + 0.009*\"amp\" + 0.009*\"need\"'), (4, '0.018*\"student\" + 0.016*\"school\" + 0.016*\"great\" + 0.012*\"today\" + 0.011*\"office\"'), (5, '0.014*\"amp\" + 0.013*\"chairman\" + 0.012*\"state\" + 0.011*\"u\" + 0.010*\"president\"'), (6, '0.016*\"live\" + 0.013*\"tune\" + 0.012*\"watch\" + 0.010*\"amp\" + 0.009*\"join\"'), (7, '0.043*\"tax\" + 0.026*\"job\" + 0.024*\"n\" + 0.021*\"american\" + 0.019*\"taxreform\"'), (8, '0.018*\"hearing\" + 0.018*\"amp\" + 0.016*\"house\" + 0.015*\"bill\" + 0.013*\"today\"'), (9, '0.025*\"family\" + 0.022*\"amp\" + 0.019*\"veteran\" + 0.015*\"happy\" + 0.014*\"day\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=10, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_LDA_model(corpus, num_topics=10):\n",
    "    \n",
    "    tokens = tokenizer.tokenize(corpus)\n",
    "    \n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    texts = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "    new_corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(new_corpus, num_topics=num_topics, id2word = dictionary, passes=20)\n",
    "    \n",
    "    \n",
    "    return ldamodel\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_topics(ldaModel, num_topics=10):\n",
    "    \n",
    "    print(ldamodel.print_topics(num_topics=num_topics, num_words=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-299-e79e239b0f19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_LDA_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_r\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-298-83d5bbfc8a8f>\u001b[0m in \u001b[0;36mgenerate_LDA_model\u001b[0;34m(corpus, num_topics)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp_stemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopped_tokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnew_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\PSpankay\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\PSpankay\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[1;31m# update Dictionary with the document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         logger.info(\n",
      "\u001b[0;32mC:\\Users\\PSpankay\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[1;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "lda_r = generate_LDA_model(corpus_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial testing and model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tweets_df.drop(['politics', 'tweets_clean', 'tweets_raw'])\n",
    "y = tweets_df['politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
