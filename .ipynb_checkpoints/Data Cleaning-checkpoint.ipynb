{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Authentication and setup for Tweepy\n",
    "\n",
    "consumer_key = 'aQzHCKeyTaBKwqJaaiwda4bkO'\n",
    "consumer_secret = 'JLRfDVG7k8GbbkkOZofL6Al69P2Ov1xhWRBAjFNFR4FwSKoXDj'\n",
    "access_token = '3125452793-u04kuuehiQIPh94QUelRV6RREMfOxoDBq7oznT7'\n",
    "access_token_secret = 'xcejx5JhpsvRvsWylOjXbeKyUce8AjxgsmFz54v5qdvrh'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "handles = pd.read_csv('TwitterHandles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function which, given a set of tweets from Tweepy API, will write them to a csv file\n",
    "\n",
    "def write_tweets_to_csv(tweets, csvWriter, political_affiliation):\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        csvWriter.writerow([tweet.user.screen_name, tweet.full_text.encode('utf-8'), political_affiliation])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for retrieving latest tweets of a given username\n",
    "\n",
    "def get_user_tweets(username, tweet_count = 200): \n",
    "    \n",
    "    tweets = api.user_timeline(screen_name = username, count = tweet_count, \n",
    "                          include_rts = False, tweet_mode='extended')\n",
    "    return tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main function for recording tweets from an account\n",
    "\n",
    "def record_tweets(username, csvWriter, politic):\n",
    "    \n",
    "    tweets = get_user_tweets(username)\n",
    "    \n",
    "    write_tweets_to_csv(tweets, csvWriter, politic)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a dataframe of politician usernames will c\n",
    "\n",
    "def get_tweets_from_dataframe(dataframe, csvWriter):\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        record_tweets(row['TwitterHandle'], csvWriter, row['Party'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a csv file\n",
    "csvFile = open('tweets_final3.csv', 'a')\n",
    "\n",
    "#Use csv Writer\n",
    "csvWriter = csv.writer(csvFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_tweets_from_dataframe(handles, csvWriter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the csv into a dataframe\n",
    "\n",
    "tweets_df = pd.read_csv('tweets_final3.csv', names = [\"username\", \"tweets_raw\", \"politics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Various methods for cleaning tweet strings to make them better suited for sentiment analysis\n",
    "\n",
    "def remove_emojis(string):\n",
    "    \n",
    "    corrected = re.sub(r'\\\\x[\\w+]{2}', '', string)\n",
    "    \n",
    "    return corrected\n",
    "\n",
    "def correct_apostrophes(string):\n",
    "    \n",
    "    corrected = re.sub(r'\\\\xe2\\\\x80\\\\x99', '\\'', string)\n",
    "    \n",
    "    corrected = re.sub(r'\\&amp;', 'and', string)\n",
    "    \n",
    "    return corrected\n",
    "\n",
    "def remove_byte_encoding(string):\n",
    "    return string[2:]\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \n",
    "            \n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        return re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    \n",
    "        parts = tweet.split('.')\n",
    "        \n",
    "        for i in range(len(parts)):\n",
    "            parts[i] = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", parts[i])\n",
    "            \n",
    "        cleaned_tweet = \" \".join(parts)\n",
    "        \n",
    "        return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_string(string):\n",
    "    \n",
    "    temp = remove_byte_encoding(string)\n",
    "    temp = correct_apostrophes(string)\n",
    "    temp = remove_emojis(temp)\n",
    "    final = clean_tweet(temp)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_dataframe_tweets(dataframe):\n",
    "    \n",
    "    new_strings = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        new_strings.append(clean_string(row['tweets_raw']))\n",
    "    \n",
    "    return new_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned = clean_dataframe_tweets(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df['tweets_clean'] = cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiments, hashtags, and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'Touching and; productive conversation w/ Florida student survivors of gun violence. We\\\\xe2\\\\x80\\\\x99re listening. And agree we need ACTION to stop this epidemic! I stand with our young leaders in support of sensible gun laws. \\\\n#NeverAgain https://t.co/gCiOtA6qeF'\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_apostrophes(tweets_df['tweets_raw'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'Touching &amp; productive conversation w/ Florida student survivors of gun violence. We\\\\xe2\\\\x80\\\\x99re listening. And agree we need ACTION to stop this epidemic! I stand with our young leaders in support of sensible gun laws. \\\\n#NeverAgain https://t.co/gCiOtA6qeF'\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['tweets_raw'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracts all hashtags from a string. Useful for getting hashtags from tweets\n",
    "\n",
    "def extract_hashtags(string):\n",
    "    hashtags = re.findall(r\"#(\\w+)\", string)\n",
    "    \n",
    "    return hashtags\n",
    "\n",
    "\n",
    "# Iterates over a dataframe and extracts all hashtags\n",
    "\n",
    "def find_hashtags(dataframe):\n",
    "    \n",
    "    h_list = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        hashtags = extract_hashtags(row['tweets_raw'])\n",
    "        \n",
    "        h_list.append(hashtags)\n",
    "    \n",
    "    return h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracts all hashtags from a string. Useful for getting hashtags from tweets\n",
    "\n",
    "def extract_mentions(string):\n",
    "    hashtags = re.findall(r\"@(\\w+)\", string)\n",
    "    \n",
    "    return hashtags\n",
    "\n",
    "\n",
    "# Iterates over a dataframe and extracts all hashtags\n",
    "\n",
    "def find_mentions(dataframe):\n",
    "    \n",
    "    h_list = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        hashtags = extract_mentions(row['tweets_raw'])\n",
    "        \n",
    "        h_list.append(hashtags)\n",
    "    \n",
    "    return h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract hashtags and add them as a separate column\n",
    "\n",
    "hashtags = find_hashtags(tweets_df)\n",
    "\n",
    "tweets_df['hashtags'] = hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract mentions and add them as a separate column\n",
    "\n",
    "mentions = find_mentions(tweets_df)\n",
    "\n",
    "tweets_df['mentions'] = mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sentiment(string):\n",
    "    text = TextBlob(string)\n",
    "    \n",
    "    return text.sentiment\n",
    "\n",
    "def get_dataframe_sentiments(dataframe):\n",
    "    \n",
    "    sentiments = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        sentiments.append(get_sentiment(row['tweets_clean']))\n",
    "        \n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiments = get_dataframe_sentiments(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>tweets_raw</th>\n",
       "      <th>politics</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>tweets_clean</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Touching &amp;amp; productive conversation w/ Flor...</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>[NeverAgain]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Touching amp productive conversation w Florida...</td>\n",
       "      <td>(0.2333333333333333, 0.3333333333333333)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Great news! Central FL's @OrangeCoSheriff awar...</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>[Sayfie]</td>\n",
       "      <td>[OrangeCoSheriff, fema]</td>\n",
       "      <td>Great news Central FL s awarded over 1 million...</td>\n",
       "      <td>(0.3416666666666666, 0.48333333333333334)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>2016 Fla elections were subject to cyberattack...</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>[Sayfie]</td>\n",
       "      <td>[HispanicCaucus, HouseNewDems]</td>\n",
       "      <td>2016 Fla elections were subject to cyberattack...</td>\n",
       "      <td>(-0.08333333333333333, 0.19999999999999998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Enjoyed assembling snack packages with colleag...</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>[BeTheForce]</td>\n",
       "      <td>[the_USO]</td>\n",
       "      <td>Enjoyed assembling snack packages with colleag...</td>\n",
       "      <td>(0.35000000000000003, 0.6166666666666667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Puerto Rico needs a long-term solution! CMS mu...</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>[HurricaneSeason2018]</td>\n",
       "      <td>[Pwr4PuertoRico, HispanicCaucus, HispanicFed, ...</td>\n",
       "      <td>Puerto Rico needs a long term solution CMS mus...</td>\n",
       "      <td>(0.225, 0.45)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        username                                         tweets_raw  politics  \\\n",
       "0  RepDarrenSoto  Touching &amp; productive conversation w/ Flor...  Democrat   \n",
       "1  RepDarrenSoto  Great news! Central FL's @OrangeCoSheriff awar...  Democrat   \n",
       "2  RepDarrenSoto  2016 Fla elections were subject to cyberattack...  Democrat   \n",
       "3  RepDarrenSoto  Enjoyed assembling snack packages with colleag...  Democrat   \n",
       "4  RepDarrenSoto  Puerto Rico needs a long-term solution! CMS mu...  Democrat   \n",
       "\n",
       "                hashtags                                           mentions  \\\n",
       "0           [NeverAgain]                                                 []   \n",
       "1               [Sayfie]                            [OrangeCoSheriff, fema]   \n",
       "2               [Sayfie]                     [HispanicCaucus, HouseNewDems]   \n",
       "3           [BeTheForce]                                          [the_USO]   \n",
       "4  [HurricaneSeason2018]  [Pwr4PuertoRico, HispanicCaucus, HispanicFed, ...   \n",
       "\n",
       "                                        tweets_clean  \\\n",
       "0  Touching amp productive conversation w Florida...   \n",
       "1  Great news Central FL s awarded over 1 million...   \n",
       "2  2016 Fla elections were subject to cyberattack...   \n",
       "3  Enjoyed assembling snack packages with colleag...   \n",
       "4  Puerto Rico needs a long term solution CMS mus...   \n",
       "\n",
       "                                     sentiment  \n",
       "0     (0.2333333333333333, 0.3333333333333333)  \n",
       "1    (0.3416666666666666, 0.48333333333333334)  \n",
       "2  (-0.08333333333333333, 0.19999999999999998)  \n",
       "3    (0.35000000000000003, 0.6166666666666667)  \n",
       "4                                (0.225, 0.45)  "
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df_d = tweets_df.head(n=33099)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df_r = tweets_df.iloc[33099:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct corpus from the tweets, grouped by politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = tweets_df['tweets_clean']\n",
    "s.str.cat(sep='. ')\n",
    "\n",
    "corpus = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = tweets_df_d['tweets_clean']\n",
    "s.str.cat(sep='. ')\n",
    "\n",
    "corpus_d = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = tweets_df_r['tweets_clean']\n",
    "\n",
    "corpus_r = s.str.cat(sep='. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in tweets_df_r['tweets_clean']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the object for LDA model\n",
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6f5bf3a20acb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'# Running and Trainign LDA model on the document term matrix\\nldamodel = Lda(doc_term_matrix, num_topics=10, id2word = dictionary, passes=1)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\PSpankay\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\PSpankay\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\PSpankay\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Lda' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Running and Trainign LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word = dictionary, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.020*\"bill\" + 0.020*\"act\" + 0.015*\"house\" + 0.012*\"h\" + 0.011*\"today\"'), (1, '0.017*\"thank\" + 0.016*\"law\" + 0.012*\"officer\" + 0.010*\"enforcement\" + 0.010*\"community\"'), (2, '0.021*\"great\" + 0.011*\"thanks\" + 0.010*\"today\" + 0.010*\"forward\" + 0.008*\"meeting\"'), (3, '0.011*\"government\" + 0.010*\"health\" + 0.010*\"national\" + 0.009*\"amp\" + 0.009*\"need\"'), (4, '0.018*\"student\" + 0.016*\"school\" + 0.016*\"great\" + 0.012*\"today\" + 0.011*\"office\"'), (5, '0.014*\"amp\" + 0.013*\"chairman\" + 0.012*\"state\" + 0.011*\"u\" + 0.010*\"president\"'), (6, '0.016*\"live\" + 0.013*\"tune\" + 0.012*\"watch\" + 0.010*\"amp\" + 0.009*\"join\"'), (7, '0.043*\"tax\" + 0.026*\"job\" + 0.024*\"n\" + 0.021*\"american\" + 0.019*\"taxreform\"'), (8, '0.018*\"hearing\" + 0.018*\"amp\" + 0.016*\"house\" + 0.015*\"bill\" + 0.013*\"today\"'), (9, '0.025*\"family\" + 0.022*\"amp\" + 0.019*\"veteran\" + 0.015*\"happy\" + 0.014*\"day\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=10, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_LDA_model(corpus, num_topics=10):\n",
    "    \n",
    "    tokens = tokenizer.tokenize(corpus)\n",
    "    \n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    texts = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "    new_corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(new_corpus, num_topics=num_topics, id2word = dictionary, passes=20)\n",
    "    \n",
    "    \n",
    "    return ldamodel\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_topics(ldaModel, num_topics=10):\n",
    "    \n",
    "    print(ldamodel.print_topics(num_topics=num_topics, num_words=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial testing and model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tweets_df.drop(['politics', 'tweets_clean', 'tweets_raw'])\n",
    "y = tweets_df['politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
